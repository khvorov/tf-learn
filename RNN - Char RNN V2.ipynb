{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "import ptb_iterator as reader\n",
    "\n",
    "plt.rc('figure', figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "    file_name = 'tinyshakespeare.txt'\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        urllib.request.urlretrieve(file_url, file_name)\n",
    "else:\n",
    "#    file_name = 'majakovski.txt'\n",
    "    file_name = 'vm.txt' # Leo Tolstoy\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print('Data lenght: {} bytes'.format(len(raw_data)))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "# cross-validation set size in percents of the dataset size \n",
    "cv_size = int(len(raw_data) * 0.05)\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "\n",
    "del raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def gen_epochs(n, seq_len, batch_size):\n",
    "    for i in range(n):\n",
    "        yield reader.ptb_iterator(data, batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn_cells(cell_type, state_size, num_layers, pkeep):\n",
    "    state_is_tuple = False\n",
    "    \n",
    "    if cell_type == 'GRU':\n",
    "        single_cell = lambda: tf.contrib.rnn.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        single_cell = lambda: tf.contrib.rnn.LSTMCell(state_size, state_is_tuple=True)\n",
    "        state_is_tuple = True\n",
    "    else:\n",
    "        single_cell = lambda: tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "\n",
    "    # wrap a cell with dropout\n",
    "    cell_creator = lambda: tf.contrib.rnn.DropoutWrapper(single_cell(), input_keep_prob=pkeep)\n",
    "\n",
    "    cells = [cell_creator() for _ in range(num_layers)]\n",
    "    return (cells, state_is_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input parameters\n",
    "CELLTYPE = 'Basic'\n",
    "SEQLEN = 4\n",
    "NCLASSES = vocab_size\n",
    "NLAYERS = 3\n",
    "INTERNALSIZE = 100\n",
    "BATCHSIZE = 64\n",
    "\n",
    "learning_rate = 1e-4\n",
    "dropout_pkeep = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholders\n",
    "lrate = tf.placeholder(tf.float32, name='learningrate')\n",
    "pkeep = tf.placeholder(tf.float32, name='pkeep')\n",
    "batchsize = tf.placeholder(tf.int32, name='batchsize')\n",
    "\n",
    "# input/output\n",
    "x = tf.placeholder(tf.int32, [None, None], name='input_placeholder') # [BATCHSIZE x SEQLEN]\n",
    "rnn_inputs = tf.one_hot(x, NCLASSES, 1.0, 0.0) # [BATCHSIZE x SEQLEN x NUMCLASSES]\n",
    "\n",
    "y = tf.placeholder(tf.int32, [None, None], name='labels_placeholder') # [BATCHSIZE x SEQLEN]\n",
    "y_oh = tf.one_hot(y, NCLASSES, 1.0, 0.0) # [BATCHSIZE x SEQLEN x NUMCLASSES]\n",
    "\n",
    "# RNN cells\n",
    "cells, state_is_tuple = create_rnn_cells(cell_type=CELLTYPE,\n",
    "                                         state_size=INTERNALSIZE,\n",
    "                                         num_layers=NLAYERS,\n",
    "                                         pkeep=pkeep)\n",
    "\n",
    "# combine them to a multicell\n",
    "multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=state_is_tuple)\n",
    "multicell = tf.contrib.rnn.DropoutWrapper(multicell, input_keep_prob=pkeep)\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [None, NLAYERS * INTERNALSIZE],\n",
    "                            name='init_state') # [BATCHSIZE x NLAYERS * INTERNALSIZE]\n",
    "\n",
    "# rnn_outputs: [BATCHSIZE x SEQLEN x INTERNALSIZE]\n",
    "# final_state: [BATCHSIZE x NLAYERS * INTERNALSIZE]\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(multicell, rnn_inputs, \n",
    "                                             dtype=tf.float32, initial_state=init_state)\n",
    "\n",
    "# just to give it a name\n",
    "final_state = tf.identity(final_state, name='final_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax layer\n",
    "rnn_outputs = tf.reshape(rnn_outputs, [-1, INTERNALSIZE]) # [BATCHSIZE * SEQLEN x INTERNALSIZE]\n",
    "logits = tf.contrib.layers.linear(rnn_outputs, NCLASSES) # [BATCHSIZE * SEQLEN x NCLASSES]\n",
    "\n",
    "y_reshaped = tf.reshape(y_oh, [-1, NCLASSES]) # [BATCHSIZE * SEQLEN x NCLASSES]\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped) # [BATCHSIZE x SEQLEN]\n",
    "loss = tf.reshape(loss, [batchsize, -1]) # [BATCHSIZE, SEQLEN]\n",
    "train_step = tf.train.AdamOptimizer(lrate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: rename it\n",
    "Yo = tf.nn.softmax(logits) # [BATCHSIZE x SEQLEN, NCLASSES]\n",
    "Y = tf.argmax(Yo, axis=1) # [BATCHSIZE x SEQLEN]\n",
    "Y = tf.reshape(Y, [batchsize, -1]) # [BATCHSIZE, SEQLEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stats for display\n",
    "seqloss = tf.reduce_mean(loss, axis=1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y, tf.cast(Y, tf.int32)), tf.float32))\n",
    "\n",
    "# summaries\n",
    "loss_summary = tf.summary.scalar('batch_loss', batchloss)\n",
    "acc_summary = tf.summary.scalar('batch_accuracy', accuracy)\n",
    "summaries = tf.summary.merge([loss_summary, acc_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summary writer & saver\n",
    "timestamp = str(math.trunc(time.time()))\n",
    "summary_writer = tf.summary.FileWriter('log/' + timestamp + '-training')\n",
    "\n",
    "# save me\n",
    "save_dir = 'checkpoints/'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init session\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Helper function to generate a sample of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(num_chars, prompt='A', pick_top_chars=None):\n",
    "    state = np.zeros([1, INTERNALSIZE * NLAYERS])\n",
    "    current_char = vocab_to_idx[prompt]\n",
    "    chars = [current_char]\n",
    "\n",
    "    for i in range(num_chars):\n",
    "        feed_dict = {x: [[current_char]], init_state: state, pkeep: 1.0, batchsize: 1}\n",
    "\n",
    "        preds, state = session.run([Yo, final_state], feed_dict)\n",
    "\n",
    "        p = np.squeeze(preds)\n",
    "\n",
    "        if pick_top_chars is not None:\n",
    "            p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "            p = p / np.sum(p)\n",
    "\n",
    "        current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "\n",
    "        chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "DISPLAY_FREQ = 200\n",
    "ND_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\n",
    "\n",
    "step = 0\n",
    "istate = np.zeros([BATCHSIZE, NLAYERS * INTERNALSIZE])\n",
    "\n",
    "for idx, epoch in enumerate(gen_epochs(1, SEQLEN, BATCHSIZE)):\n",
    "    for xe, ye in epoch:\n",
    "        if xe.shape[1] != SEQLEN or ye.shape[1] != SEQLEN:\n",
    "            continue\n",
    "\n",
    "        feed_dict = {x: xe, y: ye,\n",
    "                     init_state: istate,\n",
    "                     lrate: learning_rate,\n",
    "                     pkeep: dropout_pkeep,\n",
    "                     batchsize: BATCHSIZE\n",
    "                    }\n",
    "        _, y_, ostate, smm = session.run([train_step, Y, final_state, summaries], feed_dict=feed_dict)\n",
    "        \n",
    "        summary_writer.add_summary(smm, step)\n",
    "        \n",
    "        # display a short text generated with the current weights and biases\n",
    "        if step % ND_BATCHES == 0:\n",
    "            print('e: {0}, s: {1}: {2}'.format(idx, step,\n",
    "                                               generate_characters(256, prompt=' ', pick_top_chars=10)))\n",
    "\n",
    "        istate = ostate\n",
    "        step += BATCHSIZE * SEQLEN\n",
    "\n",
    "    print('epoch {} done'.format(idx))\n",
    "    saver.save(session, save_dir + 'rnn_train_' + timestamp, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_characters(256, prompt=' ', pick_top_chars=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
